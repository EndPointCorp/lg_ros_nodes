[bridge]
name = "stt_bridge"
input_topic = "/stt"
output_topic = "/llm/response"
queue_max = 16

[ollama]
base_url = "http://127.0.0.1:11434"
system = ""                # optional global system prompt
connect_timeout = 7.0
read_timeout = 140.0
candidate_base = "/llm"    # publishes /llm/<model> if VPNode is provided

[handler]
type = "llm_aggregator"    # "llm_aggregator" or "echo"
models = ["qwen2.5:1.5b", "llama3.2:1b", "tinyllama"]
aggregator_model = "llama3.2:1b"      # optional, omit for first-nonempty fallback
aggregator_system = ""           # optional system prompt for aggregator pass
# max_transcript_chars = 2000     # clip long ASR blobs
debug = true
max_workers = 4

[handler.candidate_options]
num_predict = 160
temperature = 0.5
top_p = 0.9
top_k = 40
repeat_penalty = 1.15
repeat_last_n = 256
num_ctx     = 4096   # keep KV small unless you need more
num_thread  = 8      # e.g., your physical cores

[handler.agg_options]
num_predict = 220
temperature = 0.4
top_p = 0.9
top_k = 40
repeat_penalty = 1.15
repeat_last_n = 256
num_ctx     = 4096   # keep KV small (1028) unless you need more
# Try Mirostat on the aggregator pass only:
# mirostat = 2
# mirostat_tau = 5.0
# mirostat_eta = 0.1


[prompts]
# must include {transcript}
prompt_template = """
[SYSTEM]
Be precise and factual. Prefer short, concrete sentences. Do not invent specifics or add any cordialities, respond with the facts, they will be parsed later.
[TRANSCRIPT START]
{transcript}
[TRANSCRIPT END]

[RESPONSE STYLE]
One clear paragraph, 1 to 5 sentences.
"""

# Aggregation template (must include {transcript} and {candidates})
aggregate_template = """
[SYSTEM]
Synthesize the best single answer from multiple candidates to address the transcript.
Resolve conflicts by preferring factual statements and internal consistency.

[TRANSCRIPT]
{transcript}

[CANDIDATES]
{candidates}

[INSTRUCTIONS]
- Produce one cohesive answer in 1 short paragraph.
- If follow-up is needed, include exactly one actionable next step.
"""

